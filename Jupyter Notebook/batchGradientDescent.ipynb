{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNzxA+ITbLsasafCGO4tDDj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# This code implements the batch_Gradient_Descent optimization algorithm based on the cost function provided by the user.\n","\n","\n","import sys\n","import numpy as np\n","import scipy as sc\n","from sympy import Symbol, diff, sympify, lambdify"],"metadata":{"id":"6GeqKmgq1HjK","executionInfo":{"status":"ok","timestamp":1702817502674,"user_tz":-210,"elapsed":393,"user":{"displayName":"Elham Jafari","userId":"09412467550778604481"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def batchGradientDescent(cost_function, f):\n","    x = Symbol('x')\n","    print(\"f(x) = \", cost_function)\n","    derivative_fx = diff(cost_function, x)\n","    print(\"df(x)/dx = \", derivative_fx)\n","    initialApproximation = float(input(\"\\n---> Enter initial approximation: \"))\n","    x0 = initialApproximation\n","    learningRate = float(input(\"---> Enter learning rate: \"))\n","    errorTolerance = float(input(\"---> Enter error tolerance: \"))\n","    print(\"\\n---------------------------------------------------------------\")\n","    print(\" *** Starting Batch Gradient Descent\")\n","    print(\"        --->  x0 =\", initialApproximation)\n","    print(\"        --->  f(x0) =\", f(initialApproximation))\n","\n","    #----------------------------------------------------------------------------------------------------\n","    numIterations = 0\n","    xk = x0\n","    while True:\n","        numIterations += 1\n","        derivative_fk = lambdify(x, derivative_fx, \"numpy\")(xk)\n","        xk = xk - learningRate * derivative_fk\n","        if abs(N(xk - x0)) < errorTolerance:\n","            break\n","\n","        x0 = xk\n","    #----------------------------------------------------------------------------------------------------\n","\n","    print(\" *** Number of Iterations =\", numIterations)\n","    print(\"        --->  Minima is at =\", xk)\n","    print(\"        --->  Minimum value of Cost Function =\", f(xk))\n","    print(\"---------------------------------------------------------------\\n\")"],"metadata":{"id":"KbeXpRMk1aj2","executionInfo":{"status":"ok","timestamp":1702817555082,"user_tz":-210,"elapsed":405,"user":{"displayName":"Elham Jafari","userId":"09412467550778604481"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Code execution section\n","\n","def main():\n","  x = Symbol('x')\n","  cost_function = input(\"---> Enter cost function f(x): \").strip()\n","  c_f = sympify(cost_function)\n","  f = lambdify(x, c_f, \"numpy\")\n","  batchGradientDescent(c_f, f)\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PRUGqs_f1gwH","executionInfo":{"status":"ok","timestamp":1702817603502,"user_tz":-210,"elapsed":10277,"user":{"displayName":"Elham Jafari","userId":"09412467550778604481"}},"outputId":"87b630dd-5fac-42c7-83f5-90cf38012a71"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["---> Enter cost function f(x): x**3 - 2*x**2 + x + 1\n","f(x) =  x**3 - 2*x**2 + x + 1\n","df(x)/dx =  3*x**2 - 4*x + 1\n","\n","---> Enter initial approximation: 1\n","---> Enter learning rate: 0.1\n","---> Enter error tolerance: 0.01\n","\n","---------------------------------------------------------------\n"," *** Starting Batch Gradient Descent\n","        --->  x0 = 1.0\n","        --->  f(x0) = 1.0\n"," *** Number of Iterations = 1\n","        --->  Minima is at = 1.0\n","        --->  Minimum value of Cost Function = 1.0\n","---------------------------------------------------------------\n","\n"]}]}]}