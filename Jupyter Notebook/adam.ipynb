{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMHzRIwIlKv3SQe7D1MtApc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# This code implements the Adam optimization algorithm based on the cost function provided by the user.\n","\n","import sys\n","import numpy as np\n","import scipy as sc\n","from sympy import Symbol, sympify"],"metadata":{"id":"z19k--d-VYpA","executionInfo":{"status":"ok","timestamp":1702675059844,"user_tz":-210,"elapsed":403,"user":{"displayName":"Elham Jafari","userId":"09412467550778604481"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["def adam(cost_function, f):\n","    x = Symbol('x')\n","    print(\"f(x) = \", cost_function)\n","    f_derivative = diff(cost_function, x)\n","    print(\"df(x)/dx = \", f_derivative)\n","    initialApproximation = float(input(\"\\n---> Enter initial approximation: \"))\n","    x_0 = initialApproximation\n","    errorTolerance = float(input(\"---> Enter error tolerance: \"))\n","    learningRate = float(input(\"---> Enter learning rate: \"))\n","    print(\"\\n---------------------------------------------------------------\")\n","    print(\" *** Starting Adam\")\n","    print(\"        --->  x0 = \", initialApproximation)\n","    print(\"        --->  f(x0) = \", f(initialApproximation))\n","\n","    #----------------------------------------------------------------------------------------------------------------------------------------------------------\n","    x_k = x_0\n","    m_0 = 0.0\n","    m_k = 0.0\n","    v_0 = 0.0\n","    v_k = 0.0\n","    b1  = 0.9\n","    b2  = 0.999\n","    x_prev  = 0.0\n","    epsilon = 10**-8\n","    iterationCount = 0\n","    while True:\n","        iterationCount += 1\n","        x_prev = x_0\n","        x_0 = x_k\n","        m_0 = m_k\n","        v_0 = v_k\n","        f_derivative_k = (lambdify(x, f_derivative, \"numpy\"))(x_k)                                    # Computing the derivative of f at x_k\n","        gradient_t     = f_derivative_k\n","        m_k  = b1 * m_0 + (1 - b1) * gradient_t                                                       # Updating m_k and v_k using exponential moving averages\n","        v_k  = b2 * v_0 + (1 - b2) * (gradient_t ** 2)\n","        mc_k = m_k / (1 - b1 ** iterationCount)                                                       # Bias correction\n","        vc_k = v_k / (1 - b2 ** iterationCount)\n","        x_k  = x_k - learningRate * mc_k / ((vc_k ** 0.5) + epsilon)                                  # Updating x_k using the Adam optimization algorithm\n","        if abs(N(x_k - x_0)) < errorTolerance or abs(N(x_k - x_prev)) < 0.1 * errorTolerance:         # Checking termination conditions\n","            break\n","    #----------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","    print(\" *** Number of Iterations = \", iterationCount)\n","    print(\"        --->  Minima is at = \", x_k)\n","    print(\"        --->  Minimum value of Cost Function = \", f(x_k))\n","    print(\"---------------------------------------------------------------\\n\")\n"],"metadata":{"id":"rtRJs1uSV8Yp","executionInfo":{"status":"ok","timestamp":1702675076161,"user_tz":-210,"elapsed":441,"user":{"displayName":"Elham Jafari","userId":"09412467550778604481"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["# Code execution section\n","\n","def main():\n","    x = Symbol('x')\n","    cost_function = input(\"---> Enter cost function f(x): \").strip()\n","    cost_function_sympy = sympify(cost_function)\n","    f = lambdify(x, cost_function_sympy, \"numpy\")\n","    adam(cost_function_sympy, f)\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oXecFMppV_HO","executionInfo":{"status":"ok","timestamp":1702675152617,"user_tz":-210,"elapsed":11151,"user":{"displayName":"Elham Jafari","userId":"09412467550778604481"}},"outputId":"c0abae9b-6a9c-4b5b-9778-24119cb5ed75"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["---> Enter cost function f(x): x**2 + 2*x + 1\n","f(x) =  x**2 + 2*x + 1\n","df(x)/dx =  2*x + 2\n","\n","---> Enter initial approximation: 0\n","---> Enter error tolerance: 0.001\n","---> Enter learning rate: 0.01\n","\n","---------------------------------------------------------------\n"," *** Starting Adam\n","        --->  x0 =  0.0\n","        --->  f(x0) =  1.0\n"," *** Number of Iterations =  177\n","        --->  Minima is at =  -0.9674397511701783\n","        --->  Minimum value of Cost Function =  0.0010601698038599228\n","---------------------------------------------------------------\n","\n"]}]}]}