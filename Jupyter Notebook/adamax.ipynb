{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPvGRcyrvugh78wf933ln1b"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# This code implements the Adamax optimization algorithm based on the cost function provided by the user.\n","\n","import sys\n","import numpy as np\n","import scipy as sc\n","from sympy import Symbol, diff, lambdify, sympify"],"metadata":{"id":"iZ_98n8Mc5Rg","executionInfo":{"status":"ok","timestamp":1702676966584,"user_tz":-210,"elapsed":416,"user":{"displayName":"Elham Jafari","userId":"09412467550778604481"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def adamax(cost_function, function):\n","    x = Symbol('x')\n","    print(\"f(x) = \", cost_function)\n","    derivative = diff(cost_function, x)\n","    print(\"df(x)/dx = \", derivative)\n","    initialApproximation = float(input(\"\\n---> Enter initial approximation: \"))\n","    x0 = initialApproximation\n","    errorTolerance = float(input(\"---> Enter error tolerance: \"))\n","    learningRate = 0.002\n","    print(\"\\n---------------------------------------------------------------\")\n","    print(\" *** Starting AdaMax\")\n","    print(\"        ---> x0 = \", initialApproximation)\n","    print(\"        ---> f(x0) = \", function(initialApproximation))\n","\n","    #----------------------------------------------------------------------------------------------------------------------------------------------------------\n","    iterationCount = 0                                                       # Keeps track of the number of iterations\n","    xk = x0                                                                   # Current value of x in each iteration\n","    x_prev = 0.0                                                              # Previous value of x\n","    m0 = 0.0                                                                  # Initial value of the first moment vector\n","    mk = 0.0                                                                  # Current value of the first moment vector\n","    v0 = 0.0                                                                  # Initial value of the second moment vector\n","    vk = 0.0                                                                  # Current value of the second moment vector\n","    beta1 = 0.9                                                               # Decay rate for the first moment vector\n","    beta2 = 0.999                                                             # Decay rate for the second moment vector\n","    epsilon = 10**-8                                                          # Small constant for numerical stability\n","    while True:\n","      iterationCount += 1\n","      x_prev = x0\n","      x0 = xk\n","      m0 = mk\n","      v0 = vk\n","      derivative_value = (lambdify(x, derivative, \"numpy\"))(xk)               # Compute the derivative value at xk\n","      gt = derivative_value                                                   # Set gt as the computed derivative value\n","      mk = beta1 * m0 + (1 - beta1) * gt                                      # Compute the updated first moment vector\n","      vk = max(beta2 * v0, gt)                                                # Compute the updated second moment vector\n","      mc_k = mk / (1 - beta1**iterationCount)                                # Compute the bias-corrected first moment estimate\n","      xk = xk - learningRate * mc_k / vk                                     # Update the value of xk using the Adam optimization formula\n","\n","      if abs(N(xk - x0)) < float(errorTolerance) or abs(N(xk - x_prev)) < 0.1 * float(errorTolerance):\n","          break\n","    #----------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","    print(\" *** Number of Iterations = \", iterationCount)\n","    print(\"        ---> Minima is at = \", xk)\n","    print(\"        ---> Minimum value of Cost Function = \", function(xk))\n","    print(\"---------------------------------------------------------------\\n\")"],"metadata":{"id":"q8MgnR-hdM5s","executionInfo":{"status":"ok","timestamp":1702677035623,"user_tz":-210,"elapsed":405,"user":{"displayName":"Elham Jafari","userId":"09412467550778604481"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# Code execution section\n","\n","def main():\n","    x = Symbol('x')\n","    cost_function = input(\"---> Enter cost function f(x): \").strip()\n","    c_f = sympify(cost_function)                                              # will lambdify c_f for fast parallel multipoint computation\n","    f = lambdify(x, c_f, \"numpy\")\n","    adamax(c_f, f)\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3QpzakNTdRPR","executionInfo":{"status":"ok","timestamp":1702677052530,"user_tz":-210,"elapsed":13670,"user":{"displayName":"Elham Jafari","userId":"09412467550778604481"}},"outputId":"19242121-cd4b-4a55-94d6-d757d930cc49"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["---> Enter cost function f(x): x**2 + 3*x + 5\n","f(x) =  x**2 + 3*x + 5\n","df(x)/dx =  2*x + 3\n","\n","---> Enter initial approximation: 2.0\n","---> Enter error tolerance: 0.001\n","\n","---------------------------------------------------------------\n"," *** Starting AdaMax\n","        ---> x0 =  2.0\n","        ---> f(x0) =  15.0\n"," *** Number of Iterations =  1729\n","        ---> Minima is at =  -1.1363286479667865\n","        ---> Minimum value of Cost Function =  2.8822568522896654\n","---------------------------------------------------------------\n","\n"]}]}]}