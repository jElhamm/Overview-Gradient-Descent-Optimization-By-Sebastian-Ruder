{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM5wW7CNusYV6Ie7MMcZYjg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["import sys\n","import numpy as np\n","import scipy as sc\n","from sympy import symbols, lambdify, diff, N"],"metadata":{"id":"0P3i8WZq9ptI","executionInfo":{"status":"ok","timestamp":1702668712615,"user_tz":-210,"elapsed":475,"user":{"displayName":"Elham Jafari","userId":"09412467550778604481"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["def adagrad(cost_function, f):\n","    x = symbols('x')\n","    print(\"cost_function(x) = \", cost_function)\n","    f_derivative = diff(cost_function, x)\n","    print(\"df(x)/dx = \", f_derivative)\n","    initialApproximation = float(input(\"\\n---> Enter initial approximation: \"))\n","    current_x = initialApproximation\n","    learningRate = float(input(\"---> Enter learning rate: \"))\n","    errorTolerance = float(input(\"---> Enter error tolerance: \"))\n","    print(\"\\n---------------------------------------------------------------\")\n","    print(\" *** Starting Adagrad\")\n","    print(\"        --->  initial x = \", initialApproximation)\n","    print(\"        ---> cost_function(initial x) = \", f(initialApproximation))\n","\n","    #----------------------------------------------------------------------------------------------------------------------------------------------------------\n","    iterationCount = 0\n","    previous_x = 0.0\n","    sumSquared_gradients = 0\n","    epsilon = 10.0 ** -8\n","    while True:\n","        iterationCount += 1\n","        previous_x = current_x\n","        fk_derivative = (lambdify(x, f_derivative, \"numpy\"))(current_x)                           # Compute the derivative for the current_x\n","        sumSquared_gradients += fk_derivative ** 2                                                # Update the sum of squared gradients\n","        adaptive_learning_rate = learningRate / ((sumSquared_gradients + epsilon) ** 0.5)         # Compute the adaptive learning rate\n","        current_x -= adaptive_learning_rate * fk_derivative                                       # Update current_x using the adaptive learning rate\n","        if abs(N(current_x - previous_x)) < errorTolerance:\n","            break\n","    #----------------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","    print(\" ***Number of Iterations = \", iterationCount)\n","    print(\"        --->  Minima is at = \", current_x)\n","    print(\"        --->  Minimum value of Cost Function = \", f(current_x))\n","    print(\"---------------------------------------------------------------\\n\")"],"metadata":{"id":"FrEd9gtP9sL8","executionInfo":{"status":"ok","timestamp":1702668942692,"user_tz":-210,"elapsed":4,"user":{"displayName":"Elham Jafari","userId":"09412467550778604481"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# Code execution section\n","\n","def main():\n","    x = Symbol('x')\n","    cost_function=input(\"---> Enter cost function f(x): \").strip()\n","    c_f=sympify(cost_function)\n","    f = lambdify(x, c_f, \"numpy\")\n","    adagrad(c_f, f)\n","\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"id":"kqVTwxKQ9yMG"},"execution_count":null,"outputs":[]}]}