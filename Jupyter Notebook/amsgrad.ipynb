{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPxDK68/LU5ycdu+5bDK6FW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["# This code implements the Amsgrad optimization algorithm based on the cost function provided by the user.\n","\n","import sys\n","import numpy as np\n","import scipy as sc\n","from sympy import Symbol, diff, lambdify, sympify"],"metadata":{"id":"qwCyw9HHhOZj","executionInfo":{"status":"ok","timestamp":1702678041392,"user_tz":-210,"elapsed":480,"user":{"displayName":"Elham Jafari","userId":"09412467550778604481"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["def nadam(cost_function, f):\n","    x = Symbol('x')\n","    print(\"f(x) = \", cost_function)\n","    f_dash = diff(cost_function, x)\n","    print(\"df(x)/dx = \", f_dash)\n","    initialApproximation = float(input(\"\\n---> Enter initial approximation: \"))\n","    x0 = initialApproximation\n","    errorTolerance = float(input(\"---> Enter error tolerance: \"))\n","    learningRate = float(input(\"---> Enter learning rate: \"))\n","    print(\"\\n---------------------------------------------------------------\")\n","    print(\" *** Starting Adam\")\n","    print(\"        --->  x0 = \", initialApproximation)\n","    print(\"        --->  f(x0) = \", f(initialApproximation))\n","\n","    #----------------------------------------------------------------------------------------------------------------------------------------------------\n","    iterationCount = 0\n","    xk = x0\n","    x_prev = 0.0\n","    m0 = 0.0\n","    mk = 0.0\n","    v0 = 0.0\n","    vk = 0.0\n","    vc_0 = 0.0\n","    vc_k = 0.0\n","    b1 = 0.9\n","    b2 = 0.999\n","    epsilon = 10 ** -8\n","    while True:\n","        iterationCount += 1\n","        x_prev = x0\n","        x0 = xk\n","        m0 = mk\n","        v0 = vk\n","        vc_0 = vc_k\n","        fk_dash = (lambdify(x, f_dash, \"numpy\"))(xk)            # Compute the derivative of f and assign it to fk_dash\n","        gt = fk_dash\n","        mk = b1 * m0 + (1 - b1) * gt                            # Update the first moment\n","        vk = b2 * v0 + (1 - b2) * (gt ** 2)                     # Update the second moment\n","        vc_k = max(vc_0, vk)\n","        xk = xk - (learningRate / (vc_k ** 0.5 + epsilon)) * mk\n","\n","        if abs(N(xk - x0)) < float(errorTolerance) or abs(N(xk - x_prev)) < 0.1 * float(errorTolerance):      # Check convergence condition\n","            break\n","    #----------------------------------------------------------------------------------------------------------------------------------------------------\n","\n","    print(\" *** Number of Iterations = \", iterationCount)\n","    print(\"        --->  Minima is at = \", xk)\n","    print(\"        --->  Minimum value of Cost Function = \", f(xk))\n","    print(\"---------------------------------------------------------------\\n\")"],"metadata":{"id":"Ae5yAsbYhXZj","executionInfo":{"status":"ok","timestamp":1702678187295,"user_tz":-210,"elapsed":451,"user":{"displayName":"Elham Jafari","userId":"09412467550778604481"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Code execution section\n","\n","def main():\n","    x = Symbol('x')\n","    cost_function = input(\"---> Enter cost function f(x): \").strip()\n","    c_f = sympify(cost_function)\n","    f = lambdify(x, c_f, \"numpy\")\n","    nadam(c_f, f)\n","\n","if __name__ == \"__main__\":\n","    main()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G87g-xAwh2kH","executionInfo":{"status":"ok","timestamp":1702678224807,"user_tz":-210,"elapsed":10772,"user":{"displayName":"Elham Jafari","userId":"09412467550778604481"}},"outputId":"98391b11-bf0f-40f4-9ba6-315f3b46e126"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["---> Enter cost function f(x): x**2 + 2*x + 1\n","f(x) =  x**2 + 2*x + 1\n","df(x)/dx =  2*x + 2\n","\n","---> Enter initial approximation: 0\n","---> Enter error tolerance: 0.001\n","---> Enter learning rate: 0.01\n","\n","---------------------------------------------------------------\n"," *** Starting Adam\n","        --->  x0 =  0.0\n","        --->  f(x0) =  1.0\n"," *** Number of Iterations =  31\n","        --->  Minima is at =  -1.1711847370990944\n","        --->  Minimum value of Cost Function =  0.029304214215686075\n","---------------------------------------------------------------\n","\n"]}]}]}